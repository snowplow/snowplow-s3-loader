# Default configuration for s3-loader

# Sources currently supported are:
# 'kinesis' for reading records from a Kinesis stream
# 'nsq' for reading records from a NSQ topic
source: "{{source}}"

# Sink is used for sending events which processing is failed.
# Sinks currently supported are:
# 'kinesis' for writing records to a Kinesis stream
# 'nsq' for writing records to a NSQ topic
sink: "{{sink}}"

# The following are used to authenticate for the Amazon Kinesis sink.
#
# If both are set to 'default', the default provider chain is used
# (see http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/auth/DefaultAWSCredentialsProviderChain.html)
#
# If both are set to 'iam', use AWS IAM Roles to provision credentials.
#
# If both are set to 'env', use environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY
aws {
  accessKey: "iam"
  secretKey: "iam"
}

# Config for NSQ
nsq {
  # Channel name for NSQ source
  channelName: "{{nsqSourceChannelName}}"
    
  # Host name for NSQ tools
  host: "{{nsqHost}}"

  # Port for nsqd
  port: "{{nsqdPort}}"

  # Port for nsqlookupd
  lookupPort: {{nsqlookupdPort}}
}

kinesis {
  # LATEST: most recent data.
  # TRIM_HORIZON: oldest available data.
  # "AT_TIMESTAMP": Start from the record at or after the specified timestamp
  # Note: This only affects the first run of this application on a stream.
  initialPosition: "{{kinesisInitialPosition}}"

  # Maximum number of records to read per GetRecords call     
  maxRecords: {{kinesisMaxRecords}}

  region: "{{kinesisRegion}}"

  # "appName" is used for a DynamoDB table to maintain stream state.
  # You can set it automatically using: "SnowplowLzoS3Sink-$\\{sink.kinesis.in.stream-name\\}"
  appName: "{{appName}}"
}

streams {
  # Input stream name
  streamNameIn = "{{InStreamName}}"

  # Stream for events for which the storage process fails
  streamNameOut = "{{OutStreamName}}"

  # Events are accumulated in a buffer before being sent to S3.
  # The buffer is emptied whenever:
  # - the combined size of the stored records exceeds byteLimit or
  # - the number of stored records exceeds recordLimit or
  # - the time in milliseconds since it was last emptied exceeds timeLimit
  buffer {
    byteLimit: {{bufferByteThreshold}} # Not supported by NSQ; will be ignored
    recordLimit: {{bufferRecordThreshold}}
    timeLimit: {{bufferTimeThreshold}} # Not supported by NSQ; will be ignored
  }
}

s3 {
  # If using us-east-1, then endpoint should be "http://s3.amazonaws.com".
  # Otherwise "http://s3-<<region>>.s3.amazonaws.com", e.g.
  # http://s3-eu-west-1.amazonaws.com
  region: "{{S3Region}}"
  bucket: "{{S3bucket}}"

  # Format is one of lzo or gzip
  # Note, that you can use gzip only for enriched data stream.
  format: "{{format}}"

  # Maximum Timeout that the application is allowed to fail for
  maxTimeout: {{maxTimeout}}
}

# Set the Logging Level for the S3 Sink
# Options: ERROR, WARN, INFO, DEBUG, TRACE
logging {
  level: "{{LogLevel}}"
}

# Optional section for tracking endpoints
monitoring {
  collectorUri: "{{collectorUri}}"
  collectorPort: 80 
  appId: "{{appName}}"
  method: "{{method}}"
}
